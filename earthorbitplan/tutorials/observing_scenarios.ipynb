{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0",
    "outputId": "227efa62-471b-4f0a-8866-065434150662"
   },
   "outputs": [],
   "source": [
    "# --- Installation & Setup (Colab only) ---\n",
    "# Clone our GitHub repository into the Colab environment\n",
    "\n",
    "import sys\n",
    "\n",
    "# Check if the env run under Colab\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    !git clone https://github.com/weizmannk/EarthOrbitPlan.git\n",
    "    %cd EarthOrbitPlan\n",
    "    !pip install -e .\n",
    "\n",
    "    print(\"Environment ready. You can now run the rest of the notebook.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "1",
    "outputId": "87389376-6127-482f-8151-257da43f1187"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Check if the env run under Colab\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Go to the \"EarthOrbitPlan/earthorbitplan/tutorials\"\n",
    "    os.chdir(\"./earthorbitplan/tutorials\")\n",
    "\n",
    "    # Check if the 'observing_scenarios.ipynb' is there\n",
    "    print(os.listdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "id": "2"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", \"Wswiglal-redir-stdio\")\n",
    "warnings.filterwarnings(\"ignore\", \".*dubious year.*\")\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", \"Tried to get polar motions for times after IERS data is valid.*\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "id": "3"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from functools import partial\n",
    "\n",
    "import astropy.units as u\n",
    "import lal.series\n",
    "import numpy as np\n",
    "import requests\n",
    "from astropy import cosmology, units\n",
    "from astropy.table import Table\n",
    "from astropy.units import dimensionless_unscaled\n",
    "from ligo.lw import ligolw\n",
    "from ligo.lw import utils as ligolw_utils\n",
    "from ligo.skymap.bayestar.filter import sngl_inspiral_psd\n",
    "from ligo.skymap.util import progress_map\n",
    "from scipy import integrate, optimize, special, stats\n",
    "from scipy.integrate import fixed_quad, quad\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.optimize import root_scalar\n",
    "\n",
    "try:\n",
    "    # From astropy >= 7.01\n",
    "    from astropy.cosmology._src.utils import vectorize_redshift_method\n",
    "except ModuleNotFoundError:\n",
    "    # between astropy 5.0 and 7.0.1\n",
    "    from astropy.cosmology._utils import vectorize_redshift_method\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {
    "id": "4"
   },
   "source": [
    "### Download and prepare GWTC-3 distribustion as farah.h5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "id": "5"
   },
   "outputs": [],
   "source": [
    "def download_file(url, filename):\n",
    "    \"\"\"Download a file from a URL and save it locally.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status()\n",
    "        with open(filename, \"wb\") as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "        return filename\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading file: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {
    "id": "6"
   },
   "source": [
    "### Define Paths and Filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7",
    "outputId": "0ae0bb09-a0f4-467e-d0c6-bc009b3faa6f"
   },
   "outputs": [],
   "source": [
    "output_dir = \"../../earthorbitplan/scenarios\"\n",
    "farah_file = os.path.join(output_dir, \"farah.h5\")\n",
    "\n",
    "# Create output_dir if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(\"cwd =\", os.getcwd())\n",
    "print(\"farah.h5 exists?\", os.path.exists(farah_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {
    "id": "8"
   },
   "source": [
    "### Download the File if Needed\n",
    "\n",
    "Download the GWTC-3 PDB file (see https://dcc.ligo.org/LIGO-T2100512/public/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "id": "9"
   },
   "outputs": [],
   "source": [
    "input_file = None\n",
    "if not os.path.exists(farah_file):\n",
    "    file_url = \"https://dcc.ligo.org/LIGO-T2100512/public/O1O2O3all_mass_h_iid_mag_iid_tilt_powerlaw_redshift_maxP_events_all.h5\"\n",
    "    file_name = os.path.join(output_dir, file_url.split(\"/\")[-1])\n",
    "    input_file = download_file(file_url, file_name)\n",
    "\n",
    "    if input_file is None:\n",
    "        logging.error(\"Failed to download the required file.\")\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {
    "id": "10"
   },
   "source": [
    "### Read, Transform, and Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "11",
    "outputId": "af10b4aa-2593-4ade-fd36-d6fe7ff25e33"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(farah_file):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "\n",
    "        # Read the original file and save only the relevant columns\n",
    "        data = Table.read()\n",
    "        Table(\n",
    "            {\n",
    "                \"mass1\": data[\"mass_1\"],\n",
    "                \"mass2\": data[\"mass_2\"],\n",
    "                \"spin1z\": data[\"a_1\"] * data[\"cos_tilt_1\"],\n",
    "                \"spin2z\": data[\"a_2\"] * data[\"cos_tilt_2\"],\n",
    "            }\n",
    "        ).write(farah_file, overwrite=True)\n",
    "\n",
    "else:\n",
    "    print(f\"farah.h5 already exists at: {farah_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {
    "id": "12"
   },
   "source": [
    "### Mass threshold for neutron stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "id": "13"
   },
   "outputs": [],
   "source": [
    "ns_mass = 3.0  # Solar masses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {
    "id": "14"
   },
   "source": [
    "### Clean Up and Print Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "15",
    "outputId": "1a2427ee-2e58-4ce7-badd-c045a250538e"
   },
   "outputs": [],
   "source": [
    "# Remove the original large file if it exists\n",
    "if input_file is not None and os.path.exists(input_file):\n",
    "    os.remove(input_file)\n",
    "    logging.info(f\"Removed temporary file: {input_file}\")\n",
    "\n",
    "# Reload the processed file\n",
    "if os.path.exists(farah_file):\n",
    "    data = Table.read(farah_file)\n",
    "\n",
    "    # Compute statistics for binary systemsÒÒ\n",
    "    n_bns = len(data[data[\"mass1\"] < ns_mass])\n",
    "    n_nsbh = len(data[(data[\"mass1\"] >= ns_mass) & (data[\"mass2\"] < ns_mass)])\n",
    "    n_bbh = len(data[data[\"mass2\"] >= ns_mass])\n",
    "\n",
    "    output = (\n",
    "        \"Summary of Compact Binary Types\\n\"\n",
    "        \"---------------------------------\\n\"\n",
    "        f\"Number of BNS   : {n_bns}\\n\"\n",
    "        f\"Number of NSBH  : {n_nsbh}\\n\"\n",
    "        f\"Number of BBH   : {n_bbh}\\n\"\n",
    "        \"---------------------------------\\n\"\n",
    "    )\n",
    "    print(output)\n",
    "else:\n",
    "    print(\"farah.h5 not found. Please check the previous steps.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {
    "id": "16"
   },
   "source": [
    "### SNR Selection Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "id": "17"
   },
   "outputs": [],
   "source": [
    "def get_decisive_snr(snrs, min_triggers):\n",
    "    \"\"\"\n",
    "    Return the SNR value that decides if an event is detectable (the min_triggers-th highest SNR).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    snrs : list or array-like\n",
    "        List of SNRs (floats).\n",
    "    min_triggers : int\n",
    "        Minimum number of triggers to form a coincidence.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    decisive_snr : float\n",
    "        The SNR of the trigger that decides detectability.\n",
    "    \"\"\"\n",
    "    return sorted(snrs)[-min_triggers]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {
    "id": "18"
   },
   "source": [
    "## Find First/Last Nonzero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {
    "id": "19"
   },
   "outputs": [],
   "source": [
    "def lo_hi_nonzero(x):\n",
    "    \"\"\"Return indices of the first and last nonzero elements of an array.\"\"\"\n",
    "    nonzero = np.flatnonzero(x)\n",
    "    return nonzero[0], nonzero[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {
    "id": "20"
   },
   "source": [
    "### GWCosmo Class: Gravitational Wave Cosmology Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {
    "id": "21"
   },
   "source": [
    "The `GWCosmo` class provides methods to calculate important cosmological figures of merit for gravitational wave (GW) astronomy.\n",
    "Given a cosmological model, it can estimate the maximum observable distance for GW sources, the sensitive volume out to a given redshift, and related quantities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {
    "id": "22"
   },
   "source": [
    "### Main Methods\n",
    "\n",
    "- `z_at_snr(...)`:  \n",
    "  Computes the redshift at which a GW source achieves a specific signal-to-noise ratio (SNR) in a detector network.\n",
    "\n",
    "- `get_max_z(...)`:  \n",
    "  Returns the maximum redshift (distance) at which a source can be detected, given the detector sensitivity and source parameters.\n",
    "\n",
    "- `sensitive_volume(z)`:  \n",
    "  Calculates the sensitive comoving volume out to redshift `z`, useful for GW event rate calculations.\n",
    "\n",
    "- `sensitive_distance(z)`:  \n",
    "  Returns the “sensitive distance,” defined such that the sensitive volume equals that of a sphere with radius `d_s(z)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {
    "id": "23"
   },
   "outputs": [],
   "source": [
    "class GWCosmo:\n",
    "    \"\"\"Evaluate GW distance figures of merit for a given cosmology.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cosmo : :class:`astropy.cosmology.FLRW`\n",
    "        The cosmological model.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cosmology):\n",
    "        self.cosmo = cosmology\n",
    "\n",
    "    def z_at_snr(\n",
    "        self,\n",
    "        psds,\n",
    "        waveform,\n",
    "        f_low,\n",
    "        snr_threshold,\n",
    "        min_triggers,\n",
    "        mass1,\n",
    "        mass2,\n",
    "        spin1z,\n",
    "        spin2z,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Get redshift at which a waveform attains a given SNR.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        psds : list\n",
    "            List of :class:`lal.REAL8FrequencySeries` objects.\n",
    "        waveform : str\n",
    "            Waveform approximant name.\n",
    "        f_low : float\n",
    "            Low-frequency cutoff for template.\n",
    "        snr_threshold : float\n",
    "            Minimum single-detector SNR.\n",
    "        min_triggers : int\n",
    "            Minimum number of triggers to form a coincidence.\n",
    "        params : list\n",
    "            List of waveform parameters: mass1, mass2, spin1z, spin2z.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        comoving_distance : float\n",
    "            Comoving distance in Mpc.\n",
    "\n",
    "        \"\"\"\n",
    "        # Construct waveform\n",
    "        series = sngl_inspiral_psd(\n",
    "            waveform,\n",
    "            f_low=f_low,\n",
    "            mass1=mass1,\n",
    "            mass2=mass2,\n",
    "            spin1z=spin1z,\n",
    "            spin2z=spin2z,\n",
    "        )\n",
    "        i_lo, i_hi = lo_hi_nonzero(series.data.data)\n",
    "        log_f = np.log(series.f0 + series.deltaF * np.arange(i_lo, i_hi + 1))\n",
    "        log_f_lo = log_f[0]\n",
    "        log_f_hi = log_f[-1]\n",
    "        num = interp1d(\n",
    "            log_f,\n",
    "            np.log(series.data.data[i_lo : i_hi + 1]),\n",
    "            fill_value=-np.inf,\n",
    "            bounds_error=False,\n",
    "            assume_sorted=True,\n",
    "        )\n",
    "\n",
    "        denoms = []\n",
    "        for series in psds:\n",
    "            i_lo, i_hi = lo_hi_nonzero(\n",
    "                np.isfinite(series.data.data) & (series.data.data != 0)\n",
    "            )\n",
    "            log_f = np.log(series.f0 + series.deltaF * np.arange(i_lo, i_hi + 1))\n",
    "            denom = interp1d(\n",
    "                log_f,\n",
    "                log_f - np.log(series.data.data[i_lo : i_hi + 1]),\n",
    "                fill_value=-np.inf,\n",
    "                bounds_error=False,\n",
    "                assume_sorted=True,\n",
    "            )\n",
    "            denoms.append(denom)\n",
    "\n",
    "        def snr_at_z(z):\n",
    "            logzp1 = np.log(z + 1)\n",
    "\n",
    "            def integrand(log_f):\n",
    "                return [np.exp(num(log_f + logzp1) + denom(log_f)) for denom in denoms]\n",
    "\n",
    "            integrals, _ = fixed_quad(integrand, log_f_lo, log_f_hi - logzp1, n=1024)\n",
    "            snr = get_decisive_snr(np.sqrt(4 * integrals), min_triggers)\n",
    "            with np.errstate(divide=\"ignore\"):\n",
    "                snr /= self.cosmo.angular_diameter_distance(z).to_value(units.Mpc)\n",
    "            return snr\n",
    "\n",
    "        def root_func(z):\n",
    "            return snr_at_z(z) - snr_threshold\n",
    "\n",
    "        return root_scalar(root_func, bracket=(0, 1e3)).root\n",
    "\n",
    "    def get_max_z(\n",
    "        self,\n",
    "        psds,\n",
    "        waveform,\n",
    "        f_low,\n",
    "        snr_threshold,\n",
    "        min_triggers,\n",
    "        mass1,\n",
    "        mass2,\n",
    "        spin1z,\n",
    "        spin2z,\n",
    "        jobs=1,\n",
    "    ):\n",
    "        # Calculate the maximum distance on the grid.\n",
    "        params = [mass1, mass2, spin1z, spin2z]\n",
    "        shape = np.broadcast_shapes(*(param.shape for param in params))\n",
    "        result = list(\n",
    "            progress_map(\n",
    "                partial(\n",
    "                    self.z_at_snr, psds, waveform, f_low, snr_threshold, min_triggers\n",
    "                ),\n",
    "                *(param.ravel() for param in params),\n",
    "                jobs=jobs,\n",
    "            )\n",
    "        )\n",
    "        result = np.reshape(result, shape)\n",
    "\n",
    "        assert np.all(result >= 0), \"some redshifts are negative\"\n",
    "        assert np.all(np.isfinite(result)), \"some redshifts are not finite\"\n",
    "        return result\n",
    "\n",
    "    @vectorize_redshift_method\n",
    "    def _sensitive_volume_integral(self, z):\n",
    "        dh3_sr = self.cosmo.hubble_distance**3 / units.sr\n",
    "\n",
    "        def integrand(z):\n",
    "            result = self.cosmo.differential_comoving_volume(z)\n",
    "            result /= (1 + z) * dh3_sr\n",
    "            return result.to_value(dimensionless_unscaled)\n",
    "\n",
    "        result, _ = quad(integrand, 0, z)\n",
    "        return result\n",
    "\n",
    "    def sensitive_volume(self, z):\n",
    "        \"\"\"Sensitive volume :math:`V(z)` out to redshift :math:`z`.\n",
    "\n",
    "        Given a population of events that occur at a constant rate density\n",
    "        :math:`R` per unit comoving volume per unit proper time, the number of\n",
    "        observed events out to a redshift :math:`N(z)` over an observation time\n",
    "        :math:`T` is :math:`N(z) = R T V(z)`.\n",
    "        \"\"\"\n",
    "        dh3 = self.cosmo.hubble_distance**3\n",
    "        return 4 * np.pi * dh3 * self._sensitive_volume_integral(z)\n",
    "\n",
    "    def sensitive_distance(self, z):\n",
    "        r\"\"\"Sensitive distance as a function of redshift :math:`z`.\n",
    "\n",
    "        The sensitive distance is the distance :math:`d_s(z)` defined such that\n",
    "        :math:`V(z) = 4/3\\pi {d_s(z)}^3`, where :math:`V(z)` is the sensitive\n",
    "        volume.\n",
    "        \"\"\"\n",
    "        dh = self.cosmo.hubble_distance\n",
    "        return dh * np.cbrt(3 * self._sensitive_volume_integral(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {
    "id": "24"
   },
   "source": [
    "### Execution & Example\n",
    "\n",
    "We load the noise Power Spectral Densities (PSDs) from the XML file.\n",
    "These PSDs are used to compute signal-to-noise ratios (SNRs) across detectors.\n",
    "\n",
    "Initialize a new LIGO Light-Weight XML document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {
    "id": "25"
   },
   "outputs": [],
   "source": [
    "xmldoc = ligolw.Document()\n",
    "xmlroot = xmldoc.appendChild(ligolw.LIGO_LW())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "26",
    "outputId": "84065a6e-9a41-4169-dfde-b98962087f35"
   },
   "outputs": [],
   "source": [
    "reference_psd = f\"{output_dir}/psds.xml\"\n",
    "with open(reference_psd, \"rb\") as f:\n",
    "    xmldoc = ligolw_utils.load_fileobj(f, contenthandler=lal.series.PSDContentHandler)\n",
    "    psds = list(lal.series.read_psd_xmldoc(xmldoc).values())\n",
    "\n",
    "psds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {
    "id": "27"
   },
   "source": [
    "### Waveform model\n",
    "\n",
    "We define the waveform model, the low-frequency cutoff, and the detection thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {
    "id": "28"
   },
   "outputs": [],
   "source": [
    "waveform = \"IMRPhenomD\"  # Chosen waveform approximant\n",
    "f_low = 25.0  # Low-frequency cutoff in Hz\n",
    "snr_threshold = 1  # SNR threshold for detection\n",
    "min_triggers = 1  # Minimum number of triggers (coincident detectors)\n",
    "jobs = 1  # Number of parallel jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {
    "id": "29"
   },
   "source": [
    "\n",
    "### GWCosmo\n",
    "\n",
    "We initialize the GWCosmo object with a cosmological model (Planck15 here).\n",
    "\n",
    "This allows us to compute distances and volumes in a cosmological framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {
    "id": "30"
   },
   "outputs": [],
   "source": [
    "gwcosmo = GWCosmo(getattr(cosmology, \"Planck15\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {
    "id": "31"
   },
   "source": [
    "\n",
    "### GWTC-distribustion\n",
    "\n",
    "We load a subset (first 5) of the binary neutron star / black hole samples\n",
    "\n",
    "from an HDF5 file (`farah.h5`). Each sample includes mass1, mass2, spin1z, spin2z."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 186
    },
    "id": "32",
    "outputId": "3d34b54f-c3ca-4564-cbb8-aa63f0240d06"
   },
   "outputs": [],
   "source": [
    "distribution_samples = \"../../earthorbitplan/scenarios/farah.h5\"\n",
    "samples = Table.read(distribution_samples)[0:5]\n",
    "samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {
    "id": "33"
   },
   "source": [
    "### Maximum Redshift\n",
    "\n",
    "Compute the Maximum Redshift (`max_z`) for Each Sample\n",
    "\n",
    "For each sample, we compute the maximum redshift `max_z` at which the SNR exceeds the chosen threshold, using all available PSDs.\n",
    "This approach uses the entire PSD frequency range to ensure accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "5abcdadf7d884fe1846a7cfc112b79c5",
      "887f7fb4f3b0407dbf377f8519fa9ae1",
      "02916d485de24178aa70dd55e6eb8ae7",
      "59f60f2c61e94464ba9af994698b758e",
      "9f587d60dd76429e85d26f9152feb3d8",
      "5e8fb3489cbb4296bc4470ac344ea799",
      "aec48290735d4c5da2c60a99dacf7d44",
      "6f2484107c4946d99d47c50d095e25dc",
      "34b0a35ccb8240b58f7e3bc901e154a2",
      "f44b75c84ad549fc982cc86ff5ecdfea",
      "8335825b8c0e46b0bbf48bb3f2752ec4"
     ]
    },
    "id": "34",
    "outputId": "d5e8e44a-2cc4-49b2-8c52-8ce743e44251"
   },
   "outputs": [],
   "source": [
    "max_z = gwcosmo.get_max_z(\n",
    "    psds,\n",
    "    waveform,\n",
    "    f_low,\n",
    "    snr_threshold,\n",
    "    min_triggers,\n",
    "    samples[\"mass1\"],\n",
    "    samples[\"mass2\"],\n",
    "    samples[\"spin1z\"],\n",
    "    samples[\"spin2z\"],\n",
    "    jobs=jobs,\n",
    ")\n",
    "\n",
    "max_z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {
    "id": "35"
   },
   "source": [
    "### Comoving Sensitive Distance\n",
    "\n",
    "Convert `max_z` to Comoving Sensitive Distance\n",
    "\n",
    "We convert each value of `max_z` to a comoving sensitive distance (in Mpc) using the cosmological model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "36",
    "outputId": "2c895f60-adb9-45f7-ae47-d7caafc92ac5"
   },
   "outputs": [],
   "source": [
    "max_distance = gwcosmo.sensitive_distance(max_z).to_value(units.Mpc)\n",
    "max_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {
    "id": "37"
   },
   "source": [
    "### Probability\n",
    "\n",
    "Compute Detection Volumes and Selection Probabilities\n",
    "\n",
    "- For each sample, use the sensitive distance to calculate the corresponding detection volume:\n",
    "  \n",
    "$$\n",
    "V = \\frac{4}{3}\\pi \\times (\\text{distance})^3\n",
    "$$\n",
    "\n",
    "- These volumes are proportional to each sample's detection probability.\n",
    "- Finally, for each sample, calculate the product `V * T`, where `T` is the observation time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {
    "id": "38"
   },
   "outputs": [],
   "source": [
    "# uniform weight per sample, each distance contributes equally at first.\n",
    "probs = 1 / len(max_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {
    "id": "39"
   },
   "source": [
    "### From Equal to Volume-Weighted Probabilities\n",
    "\n",
    "- Instead of giving all distances equal probability, we weight each distance by the amount of observable space (volume) at that distance.\n",
    "\n",
    "- Because sources are uniformly distributed in space, a larger volume means a higher chance of containing a source.\n",
    "- Thus, the probability of detecting a source increases with the volume we can observe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "40",
    "outputId": "929fc4f3-659a-4901-e882-100cfe95d2b3"
   },
   "outputs": [],
   "source": [
    "probs *= 4 / 3 * np.pi * max_distance**3\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {
    "id": "41"
   },
   "source": [
    "### Step: Normalize the Probabilities\n",
    "\n",
    "To obtain a valid probability distribution:\n",
    "\n",
    "- We divide each probability (or volume) by the total sum, so that the sum of all probabilities is exactly 1.\n",
    "\n",
    "- This way, the probabilities represent a proper distribution that can be used to randomly draw synthetic events according to their likelihood.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "42",
    "outputId": "de4e13d4-5229-4df2-e2c3-ff0906d3558c"
   },
   "outputs": [],
   "source": [
    "# Normalize the probabilities so they sum to 1\n",
    "volume_sum = probs.sum()\n",
    "print(\"Sum of probabilities (before normalization):\", volume_sum)\n",
    "\n",
    "probs /= volume_sum  # Element-wise division\n",
    "\n",
    "print(\"\\nNormalized probabilities:\\n\", probs)\n",
    "print(\"\\nSum of normalized probabilities:\", probs.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {
    "id": "43"
   },
   "source": [
    "### Samples\n",
    "\n",
    "Simulate events by drawing samples according to their normalized probabilities, using either weighted sampling or a weighted random distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "44",
    "outputId": "8ef2e830-8e5a-44dd-8fc6-16c9921586b6"
   },
   "outputs": [],
   "source": [
    "nsamples = len(samples)\n",
    "dist = stats.rv_discrete(values=(np.arange(len(probs)), probs))\n",
    "\n",
    "nsamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "45",
    "outputId": "0223dbb7-114d-432d-e6db-7b4d38395bfa"
   },
   "outputs": [],
   "source": [
    "n_batches = max(nsamples * len(probs) // 1_000_000_000, 1)\n",
    "batch_sizes = [\n",
    "    len(subarray) for subarray in np.array_split(np.empty(nsamples), n_batches)\n",
    "]\n",
    "\n",
    "n_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {
    "id": "46"
   },
   "source": [
    "### Random\n",
    "\n",
    "Now, using `dist.rvs()`, we can randomly sample event indices:\n",
    "Indices corresponding to higher probabilities are more likely to be chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "47",
    "outputId": "884f7142-39da-4781-db5c-f1348910f4d6"
   },
   "outputs": [],
   "source": [
    "indices = np.concatenate([dist.rvs(size=batch_size) for batch_size in batch_sizes])\n",
    "indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {
    "id": "48"
   },
   "source": [
    "### selected properties\n",
    "\n",
    "The `cols` dictionary now contains only the selected properties (`mass1`, `mass2`, `spin1z`, `spin2z`) for the sampled events.\n",
    "\n",
    "Each entry is an array of values corresponding to your randomly drawn sample indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "49",
    "outputId": "1698a866-d3e4-40a9-8a8a-4970b943dbbc"
   },
   "outputs": [],
   "source": [
    "# Select columns for the sampled events, keeping only the desired keys\n",
    "import pandas as pd\n",
    "\n",
    "cols = {key: samples[key][indices] for key in [\"mass1\", \"mass2\", \"spin1z\", \"spin2z\"]}\n",
    "pd.DataFrame(cols)  # Display the dictionary of selected columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {
    "id": "50"
   },
   "source": [
    "### Calculate the Volumetric Rate\n",
    "\n",
    "We estimate the volumetric event rate by dividing the number of simulated events by the total sensitive volume,\n",
    "\n",
    "and express the result in units of events per year per cubic megaparsec (yr⁻¹ Mpc⁻³)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 40
    },
    "id": "51",
    "outputId": "c80ca086-8bd7-484d-9502-d5cf3d85015f"
   },
   "outputs": [],
   "source": [
    "volumetric_rate = nsamples / volume_sum * units.year**-1 * units.Mpc**-3\n",
    "volumetric_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "52",
    "outputId": "253208e5-e5da-4b38-a06a-cb576e5a37b0"
   },
   "outputs": [],
   "source": [
    "# Draw random extrinsic parameters\n",
    "distance = stats.powerlaw(a=3, scale=max_distance[indices]).rvs(size=nsamples)\n",
    "distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {
    "id": "53"
   },
   "source": [
    "# Processing with All Simulations\n",
    "\n",
    "After processing all the simulations, we will consider the new observing scenarios based on the GWTC-3 catalog.\n",
    "\n",
    "We need to standardize our rate base on the initial distribution rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "54",
    "outputId": "4ff6e7e1-3acb-4c06-90a5-788903b01137"
   },
   "outputs": [],
   "source": [
    "# Lower 5% and upper 95% quantiles of log-normal distribution for different CBC populations\n",
    "run_names = [\"O4\", \"O5\"]\n",
    "rates_table = Table(\n",
    "    [\n",
    "        # Quantiles from O3 R&P paper Table II, row 1, last column\n",
    "        {\"population\": \"BNS\", \"lower\": 100.0, \"mid\": 240.0, \"upper\": 510.0},\n",
    "        {\"population\": \"NSBH\", \"lower\": 100.0, \"mid\": 240.0, \"upper\": 510.0},\n",
    "        {\"population\": \"BBH\", \"lower\": 100.0, \"mid\": 240.0, \"upper\": 510.0},\n",
    "    ]\n",
    ")\n",
    "\n",
    "rates_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {
    "id": "55"
   },
   "source": [
    "### Splitting Compact Binary Populations\n",
    "\n",
    "To classify events as BNS (Binary Neutron Star), NSBH (Neutron Star–Black Hole), or BBH (Binary Black Hole),  \n",
    "we use an upper/lower mass limit of 3 solar masses to distinguish neutron stars (NS) from black holes (BH).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {
    "id": "56"
   },
   "outputs": [],
   "source": [
    "ns_max_mass = 3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {
    "id": "57"
   },
   "source": [
    "### Injection Summary\n",
    "\n",
    "We injected a total of **1 million** compact binary coalescences (CBCs), consisting of:\n",
    "\n",
    "- **892,762** binary neutron stars (BNS)\n",
    "\n",
    "- **35,962** neutron star–black hole binaries (NSBH)\n",
    "\n",
    "- **71,276** binary black holes (BBH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "58",
    "outputId": "69c18e7d-bfe0-4dad-c2d8-6f2a0e2563e7"
   },
   "outputs": [],
   "source": [
    "# Calculate the mass fraction for each CBC population (BNS, NSBH, BBH)\n",
    "# by dividing the population count by the total number of CBCs (1 million)\n",
    "rates_table[\"mass_fraction\"] = (\n",
    "    np.array([892762, 35962, 71276]) / 1e6\n",
    ")  # Total CBCs = 1 million\n",
    "\n",
    "\n",
    "rates_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {
    "id": "59"
   },
   "source": [
    "### === Simulated BNS Merger Rates ===\n",
    "In observing scenarios data there is a file sqlite  file \"sqlite3 events.sqlite\" where we can read the\n",
    "the simulation Merger rate, for that use this command line in terminal or import sqlite with python\n",
    "\n",
    " Use this command to retrieve comments:\n",
    "\n",
    " 1- $ sqlite3 events.sqlite\n",
    "\n",
    " 2- $ select comment from process;\n",
    "\n",
    "For example: The simulated CBC merger rate in yr^-1 Gpc^-3 for O5 and O6-HLVK configuration with (SNR = 10)\n",
    "\n",
    "From kiendrebeogo et al. 2023 the simulated rate is given by the by (yr^-1 Mpc^-3)\n",
    "\n",
    "so this need to be convert in yr^-1 Gpc^-3, before add use it here.\n",
    "\n",
    "simulation rate  sim_rate =2.712359951521142e3 (u.Gpc**-3 * u.yr**-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {
    "id": "60"
   },
   "source": [
    "### === Simulated BNS Merger Rates ===\n",
    "\n",
    "In the observing scenario data, there is an SQLite file called `events.sqlite` containing the simulated merger rates.\n",
    "\n",
    "You can access the merger rate comments using either the command line or Python:\n",
    "\n",
    "**Command-line (SQLite):**\n",
    "1. Open the database:\n",
    "\n",
    "    $ sqlite3 events.sqlite\n",
    "\n",
    "2. Retrieve the merger rate comments:\n",
    "\n",
    "    sqlite> select comment from process;\n",
    "\n",
    "\n",
    "For example: The simulated CBC merger rate in yr^-1 Gpc^-3 for O5 and O6-HLVK configuration with (SNR = 10)\n",
    "\n",
    "From kiendrebeogo et al. 2023 the simulated rate is given by the by (yr^-1 Mpc^-3)\n",
    "\n",
    "so this need to be convert in yr^-1 Gpc^-3, before add use it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "61",
    "outputId": "2111f372-4fe2-476a-9ddb-890968ddb595"
   },
   "outputs": [],
   "source": [
    "# Here BNS, BBH and NSBH have the same simulation rates as they have been simulated together.\n",
    "rates_table[\"sim_rate_O4\"] = [6.5462076e-06, 6.5462076e-06, 6.5462076e-06] * (\n",
    "    1 / (u.Mpc**3 * u.yr)\n",
    ")\n",
    "rates_table[\"sim_rate_O5\"] = [2.71235995e-06, 2.71235995e-06, 2.71235995e-06] * (\n",
    "    1 / (u.Mpc**3 * u.yr)\n",
    ")\n",
    "\n",
    "# Conversion in Gpc^-3/ yr\n",
    "rates_table[\"sim_rate_O4\"] = rates_table[\"sim_rate_O4\"].to(u.Gpc**-3 * u.yr**-1)\n",
    "rates_table[\"sim_rate_O5\"] = rates_table[\"sim_rate_O5\"].to(u.Gpc**-3 * u.yr**-1)\n",
    "\n",
    "rates_table[\"sim_rate_O4\"], rates_table[\"sim_rate_O5\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {
    "id": "62"
   },
   "source": [
    "### Add the expected number of detections for O4 and O5\n",
    "\n",
    "Here the SNR threshold to confirm a detection is 8.\n",
    "\n",
    "So all those event have a signal noise ratio > 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 165
    },
    "id": "63",
    "outputId": "c20a0810-b614-4a1a-ce34-019bd715c948"
   },
   "outputs": [],
   "source": [
    "rates_table[\"detection_number_O4\"] = np.array([1004, 184, 7070])\n",
    "rates_table[\"detection_number_O5\"] = np.array([20, 356, 9809])\n",
    "\n",
    "rates_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {
    "id": "64"
   },
   "source": [
    "### Scaling Quantiles by Mass Fraction\n",
    "\n",
    "For each CBC population, we scale the lower, median, and upper event rate quantiles by `mass_fraction` column.\n",
    "\n",
    "This can be used, for example, to focus on a sub-population or a fraction of events relevant to a specific analysis.\n",
    "\n",
    "> **Note:** Make sure that the `mass_fraction` column exists and has appropriate values (between 0 and 1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 165
    },
    "id": "65",
    "outputId": "0e8156a0-cbf4-4a6a-d267-846395babe4d"
   },
   "outputs": [],
   "source": [
    "for key in [\"lower\", \"mid\", \"upper\"]:\n",
    "    rates_table[key] *= rates_table[\"mass_fraction\"]\n",
    "\n",
    "rates_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66",
   "metadata": {
    "id": "66"
   },
   "source": [
    "### Compute Log-normal Parameters from Quantiles\n",
    "\n",
    "For each population, we calculate the mean (`mu`) and standard deviation (`sigma`) of the underlying log-normal distribution.\n",
    "- `mu` is the mean of the logarithm of the median event rate.\n",
    "- `sigma` is set so that the interval between the lower and upper quantile matches the standard width of a 90% normal interval (from 5% to 95% quantile).\n",
    "\n",
    "This is useful for statistical modeling or simulations where log-normal priors are assumed for event rates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 165
    },
    "id": "67",
    "outputId": "a02c53da-0052-46e0-bbfa-89ba61b471ad"
   },
   "outputs": [],
   "source": [
    "(standard_90pct_interval,) = np.diff(stats.norm.interval(0.9))\n",
    "rates_table[\"mu\"] = np.log(rates_table[\"mid\"])\n",
    "rates_table[\"sigma\"] = (\n",
    "    np.log(rates_table[\"upper\"]) - np.log(rates_table[\"lower\"])\n",
    ") / standard_90pct_interval\n",
    "\n",
    "rates_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "68",
    "outputId": "c7779e5e-8054-471e-916c-820fe570f7f5"
   },
   "outputs": [],
   "source": [
    "# Extract the log-normal parameters for each population as numpy arrays\n",
    "fiducial_log_rates = np.asarray(rates_table[\"mu\"])\n",
    "fiducial_log_rate_errs = np.asarray(rates_table[\"sigma\"])\n",
    "\n",
    "fiducial_log_rates, fiducial_log_rate_errs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69",
   "metadata": {
    "id": "69"
   },
   "source": [
    "### Add Log-normal Parameters to the Table\n",
    "\n",
    "For convenience, we add the log-normal mean (`fiducial_log_rate`) and standard deviation (`fiducial_log_rate_err`) as new columns in the `rates_table`.  \n",
    "This keeps all relevant parameters together and makes the table easy to use for further analysis or plotting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185
    },
    "id": "70",
    "outputId": "a089813b-d89c-423f-bd3a-2105307f2f1a"
   },
   "outputs": [],
   "source": [
    "rates_table[\"fiducial_log_rate\"] = fiducial_log_rates\n",
    "rates_table[\"fiducial_log_rate_err\"] = fiducial_log_rate_errs\n",
    "\n",
    "rates_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71",
   "metadata": {
    "id": "71"
   },
   "source": [
    "### Functions for propagating errors in rates\n",
    "\n",
    "Reproduced from https://github.com/lpsinger/observing-scenarios-simulations/blob/main/plots-and-tables.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {
    "id": "72"
   },
   "outputs": [],
   "source": [
    "def betabinom_k_n(k, n):\n",
    "    return stats.betabinom(n, k + 1, n - k + 1)\n",
    "\n",
    "\n",
    "@np.vectorize\n",
    "def poisson_lognormal_rate_cdf(k, mu, sigma):\n",
    "    lognorm_pdf = stats.lognorm(s=sigma, scale=np.exp(mu)).pdf\n",
    "\n",
    "    def func(lam):\n",
    "        prior = lognorm_pdf(lam)\n",
    "        # poisson_pdf = np.exp(special.xlogy(k, lam) - special.gammaln(k + 1) - lam)\n",
    "        poisson_cdf = special.gammaincc(k + 1, lam)\n",
    "        return poisson_cdf * prior\n",
    "\n",
    "    # Marginalize over lambda.\n",
    "    #\n",
    "    # Note that we use scipy.integrate.odeint instead\n",
    "    # of scipy.integrate.quad because it is important for the stability of\n",
    "    # root_scalar below that we calculate the pdf and the cdf at the same time,\n",
    "    # using the same exact quadrature rule.\n",
    "    cdf, _ = integrate.quad(func, 0, np.inf, epsabs=0)\n",
    "    return cdf\n",
    "\n",
    "\n",
    "@np.vectorize\n",
    "def poisson_lognormal_rate_quantiles(p, mu, sigma):\n",
    "    \"\"\"Find the quantiles of a Poisson distribution with\n",
    "    a log-normal prior on its rate.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    p : float\n",
    "        The quantiles at which to find the number of counts.\n",
    "    mu : float\n",
    "        The mean of the log of the rate.\n",
    "    sigma : float\n",
    "        The standard deviation of the log of the rate.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    k : float\n",
    "        The number of events.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    This algorithm treats the Poisson count k as a continuous\n",
    "    real variable so that it can use the scipy.optimize.root_scalar\n",
    "    root finding/polishing algorithms.\n",
    "    \"\"\"\n",
    "\n",
    "    def func(k):\n",
    "        return poisson_lognormal_rate_cdf(k, mu, sigma) - p\n",
    "\n",
    "    if func(0) >= 0:\n",
    "        return 0\n",
    "\n",
    "    result = optimize.root_scalar(func, bracket=[0, 1e6])\n",
    "    return result.root\n",
    "\n",
    "\n",
    "def format_with_errorbars(mid, lo, hi):\n",
    "    plus = hi - mid\n",
    "    minus = mid - lo\n",
    "    smallest = min(max(0, plus), max(0, minus))\n",
    "\n",
    "    if smallest == 0:\n",
    "        return str(mid), \"0\", \"0\"\n",
    "    decimals = 1 - int(np.floor(np.log10(smallest)))\n",
    "\n",
    "    if all(np.issubdtype(type(_), np.integer) for _ in (mid, lo, hi)):\n",
    "        decimals = min(decimals, 0)\n",
    "\n",
    "    plus, minus, mid = np.round([plus, minus, mid], decimals)\n",
    "    if decimals > 0:\n",
    "        fstring = \"%%.0%df\" % decimals\n",
    "    else:\n",
    "        fstring = \"%d\"\n",
    "    return [fstring % _ for _ in [mid, minus, plus]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73",
   "metadata": {
    "id": "73"
   },
   "source": [
    "### Set Probability Quantiles and Run Duration\n",
    "\n",
    "- `prob_quantiles` defines the lower (5%), median (50%), and upper (95%) quantiles used for statistical calculations.\n",
    "- `run_duration` is the duration of the observing run in years.  \n",
    "You can adjust this value if you need rates for a different observation period.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {
    "id": "74"
   },
   "outputs": [],
   "source": [
    "prob_quantiles = np.asarray([0.05, 0.5, 0.95])\n",
    "run_duration = 1.0  # years"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75",
   "metadata": {
    "id": "75"
   },
   "source": [
    "### Detection Number Quantiles Table\n",
    "\n",
    "This block computes the 5%, 50%, and 95% quantiles for the expected number of detections for each CBC population and run, based on log-normal statistics.\n",
    "\n",
    "**Table columns:**\n",
    "- `run`: Observing run (e.g., O4, O5)\n",
    "- `population`: Source type (BNS, NSBH, BBH)\n",
    "- `lo`, `mid`, `hi`: Lower, median, and upper quantiles for the expected number of detections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "76",
    "outputId": "5002e94a-4015-4f09-9145-6d5901b8d049"
   },
   "outputs": [],
   "source": [
    "results = {}\n",
    "for run_name in run_names:\n",
    "    results[run_name] = {}\n",
    "\n",
    "    for pop in [\"BNS\", \"NSBH\", \"BBH\"]:\n",
    "        rates_row = rates_table[rates_table[\"population\"] == pop]\n",
    "        rate = rates_row[f\"sim_rate_{run_name}\"] * rates_row[\"mass_fraction\"]\n",
    "\n",
    "        mu = (\n",
    "            rates_row[\"fiducial_log_rate\"]\n",
    "            + np.log(run_duration)\n",
    "            + np.log(rates_row[f\"detection_number_{run_name}\"] / rate)\n",
    "        )\n",
    "        sigma = rates_row[\"fiducial_log_rate_err\"]\n",
    "\n",
    "        lo, mid, hi = poisson_lognormal_rate_quantiles(prob_quantiles, mu, sigma)\n",
    "\n",
    "        lo = int(np.floor(lo))\n",
    "        mid = int(np.round(mid))\n",
    "        hi = int(np.ceil(hi))\n",
    "\n",
    "        mid, lo, hi = format_with_errorbars(mid, lo, hi)\n",
    "\n",
    "        results[run_name].setdefault(\"low\", {})[pop] = lo\n",
    "        results[run_name].setdefault(\"mid\", {})[pop] = mid\n",
    "        results[run_name].setdefault(\"high\", {})[pop] = hi\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "id": "77",
    "outputId": "a266f15f-e6af-4906-cf68-5e66bdb548a1"
   },
   "outputs": [],
   "source": [
    "rows = []\n",
    "for run_name, run_stats in results.items():\n",
    "    for pop in run_stats[\"mid\"]:\n",
    "        rows.append(\n",
    "            {\n",
    "                \"run\": run_name,\n",
    "                \"population\": pop,\n",
    "                \"low\": f\"-{run_stats['low'][pop]}\",\n",
    "                \"mid\": run_stats[\"mid\"][pop],\n",
    "                \"high\": f\"+{run_stats['high'][pop]}\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "results_table = Table(rows=rows, names=(\"run\", \"population\", \"low\", \"mid\", \"high\"))\n",
    "\n",
    "results_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "78",
    "outputId": "8fd5fac6-726e-4f50-b6e4-c7f7ab6ee84d"
   },
   "outputs": [],
   "source": [
    "# display by run\n",
    "for run_name in sorted(set(results_table[\"run\"])):\n",
    "    print(f\"\\n Observing Run {run_name} : Annual number of detections\")\n",
    "    print(\"=\" * 47)\n",
    "    print(results_table[results_table[\"run\"] == run_name])\n",
    "    print(\"=\" * 47)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79",
   "metadata": {
    "id": "79"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80",
   "metadata": {
    "id": "80"
   },
   "source": [
    "### For Petrov et al. 2O22 , design as LRR distribustion,\n",
    "\n",
    "the BNS , NSBH and BBH as consider as a population not a sub-pospulation from CBC ,\n",
    "\n",
    "In fact we simulate each population independently to the other ones, so 1 million each.\n",
    "\n",
    "This means there is no longer a mass fraction or simply means the mass fraction is 1 because as we simulate each population at one time.\n",
    "This"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "81",
    "outputId": "804575a3-d625-4d2c-b5b9-d60da3174118"
   },
   "outputs": [],
   "source": [
    "# Lower 5% and upper 95% quantiles of log-normal distribution for different CBC populations\n",
    "run_names = [\"O4\", \"O5\"]\n",
    "rates_table = Table(\n",
    "    [\n",
    "        # BNS rate from GWTC-2\n",
    "        # https://doi.org/10.3847/2041-8213/abe949\n",
    "        {\"population\": \"BNS\", \"lower\": 80.00, \"mid\": 320.0, \"upper\": 810.0},\n",
    "        # NSBH rate from GW200105 and GW200115 paper\n",
    "        # https://doi.org/10.3847/2041-8213/ac082e\n",
    "        {\"population\": \"NSBH\", \"lower\": 61.0, \"mid\": 130.0, \"upper\": 242.0},\n",
    "        # BBH rate from GWTC-2\n",
    "        # https://doi.org/10.3847/2041-8213/abe949\n",
    "        {\"population\": \"BBH\", \"lower\": 15.3, \"mid\": 23.9, \"upper\": 38.2},\n",
    "    ]\n",
    ")\n",
    "\n",
    "rates_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "82",
    "outputId": "7f95a3d0-75f9-4a4e-bdc3-8e4604590386"
   },
   "outputs": [],
   "source": [
    "rates_table[\"detection_number_O4\"] = np.array(\n",
    "    [1482, 2492, 6040]\n",
    ")  # np.array([1482, 2492, 6040])\n",
    "rates_table[\"detection_number_O5\"] = np.array(\n",
    "    [1482, 2492, 6040]\n",
    ")  # np.array([2307, 5441, 21559])\n",
    "\n",
    "rates_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "83",
    "outputId": "e845733e-a36c-447e-ab42-74f62a378a98"
   },
   "outputs": [],
   "source": [
    "rates_table[\"sim_rate_O4\"] = [\n",
    "    1.3598513465134647e-05,\n",
    "    4.462124683568844e-06,\n",
    "    1.355987556826069e-06,\n",
    "] * (1 / (u.Mpc**3 * u.yr))\n",
    "rates_table[\"sim_rate_O5\"] = [\n",
    "    3.908209488939608e-06,\n",
    "    1.952463943577305e-06,\n",
    "    1.0806507866022996e-06,\n",
    "] * (1 / (u.Mpc**3 * u.yr))\n",
    "\n",
    "\n",
    "# Conversion in Gpc^-3/ yr\n",
    "rates_table[\"sim_rate_O4\"] = rates_table[\"sim_rate_O4\"].to(u.Gpc**-3 * u.yr**-1)\n",
    "rates_table[\"sim_rate_O5\"] = rates_table[\"sim_rate_O5\"].to(u.Gpc**-3 * u.yr**-1)\n",
    "\n",
    "\n",
    "rates_table[\"sim_rate_O4\"], rates_table[\"sim_rate_O5\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 165
    },
    "id": "84",
    "outputId": "6c58a024-c20a-4dc5-b0bb-bdce59ac5426"
   },
   "outputs": [],
   "source": [
    "(standard_90pct_interval,) = np.diff(stats.norm.interval(0.9))\n",
    "rates_table[\"mu\"] = np.log(rates_table[\"mid\"])\n",
    "rates_table[\"sigma\"] = (\n",
    "    np.log(rates_table[\"upper\"]) - np.log(rates_table[\"lower\"])\n",
    ") / standard_90pct_interval\n",
    "\n",
    "rates_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "85",
    "outputId": "81fd87a3-31c3-4d10-f6e1-1abaa30f5338"
   },
   "outputs": [],
   "source": [
    "# Extract the log-normal parameters for each population as numpy arrays\n",
    "fiducial_log_rates = np.asarray(rates_table[\"mu\"])\n",
    "fiducial_log_rate_errs = np.asarray(rates_table[\"sigma\"])\n",
    "\n",
    "fiducial_log_rates, fiducial_log_rate_errs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 165
    },
    "id": "86",
    "outputId": "add8a6b5-866f-4a05-d508-7fd0df79be07"
   },
   "outputs": [],
   "source": [
    "rates_table[\"fiducial_log_rate\"] = fiducial_log_rates\n",
    "rates_table[\"fiducial_log_rate_err\"] = fiducial_log_rate_errs\n",
    "\n",
    "rates_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {
    "id": "87"
   },
   "outputs": [],
   "source": [
    "prob_quantiles = np.asarray([0.05, 0.5, 0.95])\n",
    "run_duration = 1.0  # years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "88",
    "outputId": "4aef0b66-9899-465a-f343-9084ff9a2145"
   },
   "outputs": [],
   "source": [
    "results = {}\n",
    "for run_name in run_names:\n",
    "    results[run_name] = {}\n",
    "\n",
    "    for pop in [\"BNS\", \"NSBH\", \"BBH\"]:\n",
    "        rates_row = rates_table[rates_table[\"population\"] == pop]\n",
    "        rate = rates_row[f\"sim_rate_{run_name}\"]  # here  rates_row['mass_fraction'] = 1\n",
    "\n",
    "        mu = (\n",
    "            rates_row[\"fiducial_log_rate\"]\n",
    "            + np.log(run_duration)\n",
    "            + np.log(rates_row[f\"detection_number_{run_name}\"] / rate)\n",
    "        )\n",
    "        sigma = rates_row[\"fiducial_log_rate_err\"]\n",
    "\n",
    "        lo, mid, hi = poisson_lognormal_rate_quantiles(prob_quantiles, mu, sigma)\n",
    "        print(lo, mid, hi)\n",
    "        lo = int(np.floor(lo))\n",
    "        mid = int(np.round(mid))\n",
    "        hi = int(np.ceil(hi))\n",
    "\n",
    "        mid, lo, hi = format_with_errorbars(mid, lo, hi)\n",
    "\n",
    "        results[run_name].setdefault(\"low\", {})[pop] = lo\n",
    "        results[run_name].setdefault(\"mid\", {})[pop] = mid\n",
    "        results[run_name].setdefault(\"high\", {})[pop] = hi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89",
   "metadata": {
    "id": "89"
   },
   "outputs": [],
   "source": [
    "rows = []\n",
    "for run_name, run_stats in results.items():\n",
    "    for pop in run_stats[\"mid\"]:\n",
    "        rows.append(\n",
    "            {\n",
    "                \"run\": run_name,\n",
    "                \"population\": pop,\n",
    "                \"low\": f\"-{run_stats['low'][pop]}\",\n",
    "                \"mid\": run_stats[\"mid\"][pop],\n",
    "                \"high\": f\"+{run_stats['high'][pop]}\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "results_table = Table(rows=rows, names=(\"run\", \"population\", \"low\", \"mid\", \"high\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "id": "90",
    "outputId": "46e90311-4585-49f3-cc50-46b5c9d2084e"
   },
   "outputs": [],
   "source": [
    "results_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "91",
    "outputId": "da268fb1-385e-4f7a-9920-4d3c7ae98465"
   },
   "outputs": [],
   "source": [
    "# display by run\n",
    "for run_name in sorted(set(results_table[\"run\"])):\n",
    "    print(f\"\\n Observing Run  {run_name}\")\n",
    "    print(\"=\" * 30)\n",
    "    print(results_table[results_table[\"run\"] == run_name])\n",
    "    print(\"=\" * 30)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "earthorbitplan_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
